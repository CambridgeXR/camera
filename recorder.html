<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Presentation Recorder</title>
<style>
  :root{--bg:#0b1120;--panel:#121a2b;--ink:#e7ecff;--muted:#a2acc4;--accent:#7aa2ff;--ok:#67f79a;--warn:#ffd166;--bad:#ff6b6b}
  html,body{margin:0;background:var(--bg);color:var(--ink);font-family:system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial,sans-serif}
  .wrap{max-width:1100px;margin:24px auto;padding:0 16px}
  h1{margin:0 0 12px;font-size:22px}
  .grid{display:grid;grid-template-columns:1.2fr .8fr;gap:16px}
  @media (max-width:900px){.grid{grid-template-columns:1fr}}
  .card{background:var(--panel);border-radius:14px;padding:12px;box-shadow:0 8px 30px rgba(0,0,0,.35)}
  .area{position:relative;aspect-ratio:16/9;background:#000;border-radius:12px;overflow:hidden}
  video,canvas{position:absolute;inset:0;width:100%;height:100%;object-fit:cover}
  .bar{display:flex;gap:8px;align-items:center;flex-wrap:wrap;margin-top:10px}
  select,button,a.btn{background:#0f1627;border:1px solid #26304a;color:var(--ink);padding:8px 12px;border-radius:10px;font-size:14px;text-decoration:none;display:inline-flex;align-items:center;gap:6px}
  button{cursor:pointer}
  button.primary{background:var(--accent);color:#0b0f1d;border:none}
  button.danger{background:var(--bad);color:#1b0b0b;border:none}
  button:disabled,select:disabled{opacity:.6;cursor:not-allowed}
  .result{display:flex;align-items:center;gap:10px;padding:10px;border-radius:10px;background:#0f1627;border:1px solid #26304a}
  .score{font-size:28px;font-weight:800}
  .tag{font-size:12px;padding:2px 8px;border-radius:8px;background:#1a2440;border:1px solid #2a3560}
  .dot{width:8px;height:8px;border-radius:50%;display:inline-block;background:#7683a8}
  .dot.live{background:#ff5a5a;box-shadow:0 0 0 6px rgba(255,90,90,.15)}
  .water{position:absolute;right:10px;bottom:10px;font-size:11px;padding:6px 8px;border-radius:999px;background:rgba(0,0,0,.45);border:1px solid rgba(255,255,255,.08)}
  .err{background:#2a1120;border:1px solid #ff6b6b;color:#ffd3d3;padding:8px 10px;border-radius:10px;display:none;white-space:pre-line}
  .hint{font-size:12px;color:#a2acc4}
  .progress{height:8px;background:#26304a;border-radius:999px;overflow:hidden;flex:1}
  .progress>span{display:block;height:100%;width:0%;background:#7aa2ff;transition:width .1s linear}
</style>

<!-- TensorFlow.js + Face Landmarks Detection (with iris) -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core@4.14.0/dist/tf-core.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter@4.14.0/dist/tf-converter.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl@4.14.0/dist/tf-backend-webgl.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection@1.0.3/dist/face-landmarks-detection.min.js"></script>
</head>
<body>
<div class="wrap">
  <h1>🎤 Presentation Recorder</h1>
  <div class="grid">
    <!-- Live + controls -->
    <div class="card">
      <div class="area">
        <video id="preview" playsinline autoplay muted></video>
        <canvas id="overlay"></canvas>
        <div class="water"><span id="liveDot" class="dot"></span> Live</div>
      </div>
      <div class="bar">
        <select id="camera"></select>
        <select id="microphone"></select>
        <label style="display:inline-flex;align-items:center;gap:6px"><input type="checkbox" id="mirror"> Mirror</label>
        <span class="tag" id="perm">Permissions: not requested</span>
      </div>
      <div class="bar">
        <button id="enable" class="primary">Enable camera & mic</button>
        <button id="start" disabled>Start</button>
        <button id="pause" disabled>Pause</button>
        <button id="resume" disabled>Resume</button>
        <button id="stop" class="danger" disabled>Stop</button>
        <a id="download" class="btn" download="practice.webm" style="display:none">Download</a>
        <span class="tag" id="timer">00:00</span>
      </div>
      <div id="err" class="err"></div>
      <div class="hint" id="hint" style="margin-top:6px;display:none"></div>
    </div>

    <!-- Playback + analysis -->
    <div class="card">
      <div class="area" style="background:#05070f">
        <video id="playback" controls playsinline></video>
        <div class="water">Recorded</div>
      </div>
      <div class="bar">
        <button id="analyze" disabled>Analyze Eye Contact</button>
        <div class="progress"><span id="prog"></span></div>
        <span class="tag" id="progText">0%</span>
      </div>
      <div class="bar">
        <div class="result" style="flex:1">
          <div>Eye contact</div>
          <div class="score" id="score">—%</div>
          <div class="mini" id="scoreDetail">no analysis yet</div>
        </div>
      </div>
      <div class="hint">
        Eye Contact % is computed after recording by sampling frames when one face is roughly front-facing (your data is not shared with anyone).
      </div>
    </div>
  </div>
</div>

<script>
(() => {
  // ---------- Elements ----------
  const v = document.getElementById('preview');
  const pb = document.getElementById('playback');
  const cv = document.getElementById('overlay');
  const cx = cv.getContext('2d');
  const camSel = document.getElementById('camera');
  const micSel = document.getElementById('microphone');
  const mirror = document.getElementById('mirror');
  const perm = document.getElementById('perm');
  const enableBtn = document.getElementById('enable');
  const startBtn = document.getElementById('start');
  const pauseBtn = document.getElementById('pause');
  const resumeBtn = document.getElementById('resume');
  const stopBtn = document.getElementById('stop');
  const downloadLink = document.getElementById('download');
  const timerEl = document.getElementById('timer');
  const errBox = document.getElementById('err');
  const hint = document.getElementById('hint');
  const liveDot = document.getElementById('liveDot');
  const analyzeBtn = document.getElementById('analyze');
  const progBar = document.getElementById('prog');
  const progText = document.getElementById('progText');
  const scoreEl = document.getElementById('score');
  const scoreDetail = document.getElementById('scoreDetail');

  // ---------- State ----------
  let stream=null, mediaRecorder=null, chunks=[];
  let recording=false, paused=false;
  let tIv=null, tStart=0;

  // FaceMesh model
  let flModel=null;

  // ---------- Utils ----------
  function showErr(msg){ errBox.textContent = msg; errBox.style.display = 'block'; }
  function clearErr(){ errBox.style.display='none'; errBox.textContent=''; }
  function fmt(s){ const m=Math.floor(s/60).toString().padStart(2,'0'); const x=Math.floor(s%60).toString().padStart(2,'0'); return `${m}:${x}`; }
  function startTimer(){ tStart=Date.now(); timerEl.textContent='00:00'; if(tIv) clearInterval(tIv); tIv=setInterval(()=>{ timerEl.textContent=fmt((Date.now()-tStart)/1000) },200); }
  function stopTimer(){ if(tIv) clearInterval(tIv); tIv=null; }
  function setCanvasSize(){ const w=v.videoWidth||cv.clientWidth, h=v.videoHeight||cv.clientHeight; if(w&&h){ cv.width=w; cv.height=h; } }

  // ---------- Devices & stream ----------
  async function listDevices(){
    const devs = await navigator.mediaDevices.enumerateDevices();
    const cams = devs.filter(d=>d.kind==='videoinput');
    const mics = devs.filter(d=>d.kind==='audioinput');
    camSel.innerHTML = cams.map(d=>`<option value="${d.deviceId}">${d.label||'Camera'}</option>`).join('') || '<option>No camera</option>';
    micSel.innerHTML = mics.map(d=>`<option value="${d.deviceId}">${d.label||'Microphone'}</option>`).join('') || '<option>No mic</option>';
  }

  // Robust permissions: try AV, fall back to video-only
  async function requestPermissionsRobust() {
    perm.textContent='Permissions: requesting…';
    try{
      const pre = await navigator.mediaDevices.getUserMedia({ video:true, audio:true });
      pre.getTracks().forEach(t=>t.stop());
      return {video:true, audio:true};
    }catch(e1){
      try{
        const pre2 = await navigator.mediaDevices.getUserMedia({ video:true, audio:false });
        pre2.getTracks().forEach(t=>t.stop());
        hint.style.display='block';
        hint.textContent='Microphone access failed or is blocked. Continuing with camera-only.';
        return {video:true, audio:false};
      }catch(e2){
        throw e1;
      }
    }
  }

  async function startStream(wantAudio=true){
    if(stream) stream.getTracks().forEach(t=>t.stop());
    const constraints = {
      video: { deviceId: camSel.value? {exact:camSel.value}:undefined, width:{ideal:1280}, height:{ideal:720} },
      audio: wantAudio ? { deviceId: micSel.value? {exact:micSel.value}:undefined, echoCancellation:true, noiseSuppression:true, autoGainControl:true } : false
    };
    stream = await navigator.mediaDevices.getUserMedia(constraints);
    v.srcObject = stream; v.muted = true; await v.play();
    liveDot.classList.add('live');
    setCanvasSize();
    drawGuide();
  }

  // ---------- Recorder ----------
  function setupRecorder(){
    chunks.length=0;
    const mime = MediaRecorder.isTypeSupported('video/webm;codecs=vp9,opus')?'video/webm;codecs=vp9,opus':
                 MediaRecorder.isTypeSupported('video/webm;codecs=vp8,opus')?'video/webm;codecs=vp8,opus':'video/webm';
    mediaRecorder = new MediaRecorder(stream,{mimeType:mime, videoBitsPerSecond:4_000_000});
    mediaRecorder.ondataavailable = e=>{ if(e.data && e.data.size) chunks.push(e.data); };
    mediaRecorder.onstop = ()=>{
      const blob = new Blob(chunks,{type: mediaRecorder.mimeType});
      const url = URL.createObjectURL(blob);
      pb.src = url;
      downloadLink.href = url;
      downloadLink.style.display='inline-flex';
      downloadLink.download = `presentation-${new Date().toISOString().replace(/[:.]/g,'-')}.webm`;
      analyzeBtn.disabled = false; // can analyze now
    };
  }

  // ---------- Overlay guide ----------
  function drawGuide(){
    requestAnimationFrame(drawGuide);
    if(!v || v.readyState < 2) return;
    setCanvasSize();
    const W=cv.width, H=cv.height;
    cx.clearRect(0,0,W,H);
    cx.save();
    if(mirror.checked){ cx.translate(W,0); cx.scale(-1,1); }
    cx.strokeStyle='rgba(122,162,255,0.5)'; cx.lineWidth=2;
    const gx=W*0.28, gy=H*0.25, gw=W*0.44, gh=H*0.5; // acceptable center window
    cx.strokeRect(gx,gy,gw,gh);
    cx.restore();
  }
  mirror.addEventListener('change', ()=>{ v.style.transform = mirror.checked? 'scaleX(-1)':'none'; });

  // ---------- FaceMesh load (once, lazy) ----------
  async function ensureFaceModel(){
    if(flModel) return;
    await tf.setBackend('webgl');
    flModel = await faceLandmarksDetection.load(
      faceLandmarksDetection.SupportedPackages.mediapipeFacemesh,
      { maxFaces: 1, shouldLoadIrisModel: true }
    );
  }

  // ---------- Helpers for gaze & pose ----------
  function bboxFromPoints(points){
    let minX=Infinity,minY=Infinity,maxX=-Infinity,maxY=-Infinity;
    for(const p of points){ const x=p[0], y=p[1]; if(x<minX)minX=x; if(y<minY)minY=y; if(x>maxX)maxX=x; if(y>maxY)maxY=y; }
    return {x:minX,y:minY,w:maxX-minX,h:maxY-minY,cx:(minX+maxX)/2,cy:(minY+maxY)/2};
  }
  function avgPoint(arr){ let x=0,y=0,n=arr.length||1; for(const p of arr){ x+=p[0]; y+=p[1]; } return [x/n,y/n]; }

  // Eye contact rule: centered+big + head yaw/pitch modest + both irises near eye centers
  function eyeContactFromAnnotations(ann, frameW, frameH){
    if(!ann?.leftEyeIris || !ann?.rightEyeIris) return false;

    const leftIrisC = avgPoint(ann.leftEyeIris);
    const rightIrisC = avgPoint(ann.rightEyeIris);

    const leftEyePts = []
      .concat(ann.leftEyeUpper0||[], ann.leftEyeLower0||[], ann.leftEyeInnerCorner||[], ann.leftEyeOuterCorner||[]);
    const rightEyePts = []
      .concat(ann.rightEyeUpper0||[], ann.rightEyeLower0||[], ann.rightEyeInnerCorner||[], ann.rightEyeOuterCorner||[]);

    const lb = bboxFromPoints(leftEyePts), rb = bboxFromPoints(rightEyePts);
    const lCenter = [lb.cx, lb.cy], rCenter = [rb.cx, rb.cy];

    // Normalized iris offset per eye (0 = center, 1 = at eyelid edge)
    const ldx = Math.abs(leftIrisC[0]-lCenter[0]) / (lb.w/2 || 1);
    const ldy = Math.abs(leftIrisC[1]-lCenter[1]) / (lb.h/2 || 1);
    const rdx = Math.abs(rightIrisC[0]-rCenter[0]) / (rb.w/2 || 1);
    const rdy = Math.abs(rightIrisC[1]-rCenter[1]) / (rb.h/2 || 1);

    const irisCentered = (ldx<0.35 && ldy<0.35 && rdx<0.35 && rdy<0.35); // stricter gaze threshold

    // Head pose & framing (use coarse bbox of full mesh)
    const facePts = []
      .concat(ann.silhouette||[], ann.leftCheek||[], ann.rightCheek||[], ann.midwayBetweenEyes||[], ann.noseTip||[]);
    if(facePts.length===0) return false;
    const fb = bboxFromPoints(facePts);

    const centerOk = (fb.cx > frameW*0.28 && fb.cx < frameW*0.72 && fb.cy > frameH*0.25 && fb.cy < frameH*0.75);
    const sizeOk   = (fb.w*fb.h)/(frameW*frameH) > 0.035;

    // Yaw/pitch heuristics from eye lines & nose relative to eye-line
    const Lc = avgPoint(ann.leftEyeUpper0||[]);
    const Rc = avgPoint(ann.rightEyeUpper0||[]);
    const eyeDx = Math.hypot((Rc[0]-Lc[0]), (Rc[1]-Lc[1])) || 1;
    const eyesLevel = Math.abs((Rc[1]-Lc[1]))/eyeDx < 0.18; // pitch/roll proxy

    const nose = (ann.noseTip && ann.noseTip[0]) ? ann.noseTip[0] : avgPoint(ann.noseBottom||ann.noseBridge||[[fb.cx,fb.cy]]);
    const earL = (ann.leftEarTragion && ann.leftEarTragion[0]) ? ann.leftEarTragion[0] : [fb.x, fb.cy];
    const earR = (ann.rightEarTragion && ann.rightEarTragion[0]) ? ann.rightEarTragion[0] : [fb.x+fb.w, fb.cy];
    const dNL = Math.hypot(nose[0]-earL[0], nose[1]-earL[1]);
    const dNR = Math.hypot(nose[0]-earR[0], nose[1]-earR[1]);
    const yawSkew = Math.abs(dNL - dNR) / ((dNL + dNR)/2 || 1);
    const yawOk = yawSkew < 0.30;

    return centerOk && sizeOk && eyesLevel && yawOk && irisCentered;
  }

  // ---------- Analyzer (post-recording) ----------
  // Fix Chrome "Infinity" duration by nudging time
  async function loadVideoForAnalysis(src){
    return new Promise((resolve, reject)=>{
      const av = document.createElement('video');
      av.preload = 'auto';
      av.muted = true;
      av.playsInline = true;
      av.src = src;

      function finish(){
        if (!isFinite(av.duration) || av.duration <= 0) {
          reject(new Error('Could not read video duration.'));
        } else {
          resolve(av);
        }
      }

      av.addEventListener('loadedmetadata', ()=>{
        if (isFinite(av.duration) && av.duration > 0 && av.duration !== Infinity) {
          finish();
        } else {
          const onTimeUpdate = ()=>{
            av.removeEventListener('timeupdate', onTimeUpdate);
            av.currentTime = 0;
            setTimeout(finish, 0);
          };
          av.addEventListener('timeupdate', onTimeUpdate);
          try { av.currentTime = 1e101; } catch(_){}
          try { av.play().then(()=>{ av.pause(); }).catch(()=>{}); } catch(_){}
        }
      }, {once:true});
      av.addEventListener('error', ()=> reject(new Error('Video could not be loaded.')), {once:true});
    });
  }

  async function analyzeEyeContact(videoURL){
    if(!videoURL){ throw new Error('No video to analyze.'); }
    await ensureFaceModel();

    const av = await loadVideoForAnalysis(videoURL);
    const W = av.videoWidth || 1280;
    const H = av.videoHeight || 720;
    const off = document.createElement('canvas'); off.width=W; off.height=H;
    const ox = off.getContext('2d');

    const duration = av.duration;
    const targetFPS = 6;                // denser sampling than before
    const totalSteps = Math.max(1, Math.floor(duration * targetFPS));
    let total=0, facing=0;

    for(let i=0;i<totalSteps;i++){
      const t = Math.min(duration - 0.05, (i/targetFPS));
      await new Promise((resolve)=>{
        const onSeek = ()=>{ av.removeEventListener('seeked', onSeek); resolve(); };
        av.addEventListener('seeked', onSeek);
        av.currentTime = t;
      });

      ox.drawImage(av, 0, 0, W, H);

      // Estimate landmarks with iris
      const preds = await flModel.estimateFaces({input: off, flipHorizontal: false, returnTensors: false, predictIrises: true});
      total++;

      if (Array.isArray(preds) && preds.length === 1 && preds[0].annotations) {
        if (eyeContactFromAnnotations(preds[0].annotations, W, H)) facing++;
      }

      const pct = Math.round((i+1)/totalSteps*100);
      progBar.style.width = pct + '%';
      progText.textContent = pct + '%';
    }

    const percent = total ? Math.round((facing/total)*100) : 0;
    scoreEl.textContent = percent + '%';
    scoreEl.style.color = percent>=70? 'var(--ok)' : percent>=40? 'var(--warn)' : 'var(--bad)';
    scoreDetail.textContent = `${facing} of ${total} sampled frames (~${targetFPS} fps)`;
  }

  // ---------- Handlers ----------
  enableBtn.addEventListener('click', async ()=>{
    clearErr();
    try{
      const got = await requestPermissionsRobust();  // prompts on click
      await listDevices();
      await startStream(!!got.audio);
      perm.textContent='Permissions: granted';
      enableBtn.disabled=true;
      startBtn.disabled=false;
    }catch(e){
      console.error(e);
      perm.textContent='Permissions: denied';
      let msg = 'Unable to access camera/microphone.\n\n';
      if(e && e.name==='NotAllowedError'){
        msg += 'Access was blocked.\nChrome: lock icon → Site settings → Allow Camera & Microphone for cambridgexr.github.io → refresh.';
      } else if(e && e.name==='NotFoundError'){
        msg += 'No camera/microphone found. Connect a device and try again.';
      } else if(e && e.name==='NotReadableError'){
        msg += 'Your camera/mic is in use by another app. Close it and retry.';
      } else if(e && e.name==='OverconstrainedError'){
        msg += 'Selected device not available. Choose another camera/mic.';
      } else {
        msg += (e && e.message) ? e.message : String(e);
      }
      showErr(msg);
      hint.style.display='block';
      hint.textContent='Windows tip: Settings → Privacy & security → Camera/Microphone → allow access for browsers.';
    }
  });

  camSel.addEventListener('change', async ()=>{ try{ await startStream(true); }catch(e){ try{ await startStream(false); }catch(_){}} });
  micSel.addEventListener('change', async ()=>{ try{ await startStream(true); }catch(e){ hint.style.display='block'; hint.textContent='That microphone failed; continuing with camera-only.'; await startStream(false); } });

  startBtn.addEventListener('click', ()=>{
    if(!stream){ showErr('Click “Enable camera & mic” first.'); return; }
    chunks.length=0;
    setupRecorder();
    mediaRecorder.start();
    recording=true; paused=false;
    startBtn.disabled=true; stopBtn.disabled=false; pauseBtn.disabled=false; resumeBtn.disabled=true;
    // simple timer
    const t0 = Date.now(); timerEl.textContent='00:00';
    tIv = setInterval(()=>{ const s=(Date.now()-t0)/1000; const m=Math.floor(s/60).toString().padStart(2,'0'); const ss=Math.floor(s%60).toString().padStart(2,'0'); timerEl.textContent=`${m}:${ss}`; }, 250);
  });

  pauseBtn.addEventListener('click', ()=>{ if(mediaRecorder && mediaRecorder.state==='recording'){ mediaRecorder.pause(); paused=true; pauseBtn.disabled=true; resumeBtn.disabled=false; }});
  resumeBtn.addEventListener('click', ()=>{ if(mediaRecorder && mediaRecorder.state==='paused'){ mediaRecorder.resume(); paused=false; pauseBtn.disabled=false; resumeBtn.disabled=true; }});
  stopBtn.addEventListener('click', ()=>{
    if(mediaRecorder && (mediaRecorder.state==='recording' || mediaRecorder.state==='paused')){
      mediaRecorder.stop(); recording=false; paused=false;
      stopBtn.disabled=true; pauseBtn.disabled=true; resumeBtn.disabled=true; startBtn.disabled=false;
      if(tIv) clearInterval(tIv);
    }
  });

  analyzeBtn.addEventListener('click', async ()=>{
    clearErr();
    try{
      analyzeBtn.disabled = true;
      progBar.style.width = '0%';
      progText.textContent = '0%';
      scoreEl.textContent = '—%';
      scoreDetail.textContent = 'analyzing…';
      await analyzeEyeContact(pb.src);
      analyzeBtn.disabled = false;
    }catch(e){
      console.error(e);
      analyzeBtn.disabled = false;
      showErr('Analysis failed: ' + (e && e.message ? e.message : e));
    }
  });

  pb.addEventListener('loadeddata', ()=>{ if(pb.src) analyzeBtn.disabled=false; });

  // Early checks
  (function(){
    if(!('mediaDevices' in navigator) || !('getUserMedia' in navigator.mediaDevices)){
      perm.textContent='Unsupported browser';
      showErr('Your browser does not support camera/microphone (no navigator.mediaDevices.getUserMedia). Use latest Chrome/Edge/Firefox.');
    }
    if(typeof MediaRecorder==='undefined'){
      showErr('Recording is not supported in this browser.');
    }
  })();
})();
</script>
</body>
</html>
